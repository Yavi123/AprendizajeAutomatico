{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c47028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======== Loading... =============== \n",
      "\n",
      "\n",
      " ===================  SIMILARITY  ===================== \n",
      "\n",
      "Vecinos más cercanos:\n",
      "[[[-0.22401113  2.40533218 -0.06294622  0.18585261  0.60490084]\n",
      "  [-0.22401113  1.71132124 -0.16463975  0.24222984  0.50889491]\n",
      "  [-0.22401113  1.51356572 -0.26135002  0.27972592  0.57493224]\n",
      "  [-0.22401113  1.52922684  0.04678098  0.20690589  0.15856397]\n",
      "  [-0.22401113  1.9973181   0.42763547  0.07368163 -0.20325688]\n",
      "  [-0.22401113  1.65982717  0.53778804  0.0176433   0.23137845]\n",
      "  [-0.22401113  1.49114722  0.13542706  0.1908588   0.03006357]]]\n",
      "Distancias:\n",
      "[[3.30725391e-09 7.10202991e-01 9.18869854e-01 9.89575906e-01\n",
      "  1.03578333e+00 1.04138020e+00 1.09797497e+00]]\n",
      "Prediccion de los vecinos mas cercanos:\n",
      "2 2 0 0 2 2 0 \n",
      "\n",
      "\n",
      " ===================  SVM  ===================== \n",
      "\n",
      "Accuracy :  72.75862068965517 %\n",
      "Matriz de Confusión:\n",
      "[[418   0  25   0   0   7   0]\n",
      " [  1   0   0   0   0   0   0]\n",
      " [168   0 194   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0]\n",
      " [ 24   0   0   0   0  21   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[418  32]\n",
      " [202 218]]\n",
      "MSE: 0.38275862068965516\n",
      "\n",
      " ===================  DECISION TREE  ===================== \n",
      "\n",
      "Accuracy :  80.11494252873564 %\n",
      "Matriz de Confusión:\n",
      "[[387   0  58   0   0   5   0]\n",
      " [  1   0   0   0   0   0   0]\n",
      " [ 73   0 289   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0]\n",
      " [ 13   0  11   0   0  21   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[387  63]\n",
      " [ 96 324]]\n",
      "MSE: 0.3471264367816092\n",
      "\n",
      " ===================  RANDOM FOREST  ===================== \n",
      "\n",
      "Accuracy :  78.27586206896552 %\n",
      "Matriz de Confusión:\n",
      "[[376   0  64   0   2   8   0]\n",
      " [  1   0   0   0   0   0   0]\n",
      " [ 75   0 286   0   0   4   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  8   0   0   0   1   0   0]\n",
      " [ 19   0   8   0   0  18   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[376  74]\n",
      " [103 317]]\n",
      "MSE: 0.367816091954023\n",
      "\n",
      " ===================  MLP  ===================== \n",
      "\n",
      "scores =  [0.07512315 0.08180884 0.06207605 0.08078109 0.07749229]\n",
      "Our MLP:\n",
      "Accuracy :  73.44827586206897 %\n",
      "Matriz de Confusión:\n",
      "[[417   0  28   0   0   5   0]\n",
      " [  0   0   1   0   0   0   0]\n",
      " [161   0 201   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0]\n",
      " [ 17   0   7   0   0  21   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[417  33]\n",
      " [187 233]]\n",
      "MSE: 0.39195402298850573\n",
      "\n",
      "\n",
      "SKL MLP:\n",
      "Accuracy :  71.26436781609196 %\n",
      "Matriz de Confusión:\n",
      "[[425   0  25   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0]\n",
      " [170   0 195   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0]\n",
      " [ 45   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[425  25]\n",
      " [225 195]]\n",
      "MSE: 0.3873563218390805\n"
     ]
    }
   ],
   "source": [
    "# Ignacio Del Castillo Rubio\n",
    "# Javier Villegas Montelongo\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ann import costL2, backprop, forwardprop, iterateThetas\n",
    "\n",
    "from Utils import ExportThetasToFile\n",
    "from Utils import CreateTxt\n",
    "print(\"\\n ======== Loading... =============== \\n\" )\n",
    "#print(pd.__version__)\n",
    "\n",
    "# EJERCICIO 2 - visualiza los datos\n",
    "\n",
    "# Cargar los datos desde el archivo CSV\n",
    "data = pd.read_csv(\"data/kartAllData.csv\", header=None, names=[\"dist1\", \"dist2\", \"dist3\", \"dist4\", \"dist5\", \n",
    "                                                           \"x\", \"y\", \"z\", \"tiempo\", \"accion\"])\n",
    "\n",
    "# Eliminar las columnas de posición y tiempo ya que consideramos que no son tan relevantes como la distancia de los raycast\n",
    "droppingPosition = True\n",
    "\n",
    "if (droppingPosition):\n",
    "    cleanDataset = data.drop([\"x\", \"y\", \"z\", \"tiempo\"], axis=1)\n",
    "    numInputs = 5\n",
    "else:\n",
    "    cleanDataset = data.drop([\"y\", \"tiempo\"], axis=1)\n",
    "    numInputs = 7\n",
    "    \n",
    "# Realizar one-hot encoding para la columna \"accion\"\n",
    "one_hot_encoded = pd.get_dummies(data[\"accion\"], prefix=\"accion\")\n",
    "\n",
    "# Añadir los valores generados del one_hot_encoded al dataset\n",
    "cleanDataset = pd.concat([cleanDataset, one_hot_encoded], axis=1)\n",
    "cleanDataset = cleanDataset.drop([\"accion\"], axis=1)\n",
    "\n",
    "# Utilizaremos los valores de las distancias de los 5 raycast como input\n",
    "X = cleanDataset.iloc[:, :numInputs]\n",
    "# Mientras que utilizaremos los 7 estados de accion, pasados por one_hot_encoded\n",
    "Y = cleanDataset.iloc[:, numInputs:]\n",
    "\n",
    "#==Normalizacion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaling=StandardScaler()\n",
    "scaling.fit(X)\n",
    "X=scaling.transform(X)\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "#                    DIVIDIR DATOS ENTRE ENTRENAMIENTO Y DE TEST\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, train_size = 0.8, random_state = 1234)\n",
    "\n",
    "\n",
    "# Visualizar la distribución de clases\n",
    "class_distribution = data[\"accion\"].value_counts()\n",
    "\n",
    "# Graficar la distribución de clases\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribución de Clases')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Frecuencia')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# Los valores de Y_pred y Y_test tienen que tener el siguiente formato:\n",
    "# Y_pred =  [0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0  ... 0 0 0 0 0 2 0] \n",
    "def PrintData(Y_pred, Y_test) :\n",
    "    # Calcula el accuracy\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    print(\"Accuracy : \", acc * 100, \"%\")\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    Y_test_np = np.array(Y_test, dtype=int)\n",
    "    Y_pred_np = np.array(Y_pred, dtype=int)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(Y_test_np, Y_pred_np, labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "    \n",
    "    print(\"Matriz de Confusión:\")\n",
    "    print(conf_matrix)\n",
    "    Y_test_np = np.array(Y_test, dtype=bool)\n",
    "    Y_pred_np = np.array(Y_pred, dtype=bool)\n",
    "    \n",
    "    conf_matrix_bool = confusion_matrix(Y_test_np.flatten(), Y_pred_np.flatten())\n",
    "    \n",
    "    print(\"Matriz de Confusión en bools:\")\n",
    "    print(conf_matrix_bool)\n",
    "    \n",
    "    # Para calcular el MSE de forma mas coherente, hemos decidido mover estos comandos en este orden \n",
    "    # LEFT_ACCELERATE, ACCELERATE, RIGHT_ACCELERATE\n",
    "    # Para que el error sea mayor entre las direcciones laterales, pero no tan grande respecto al medio\n",
    "    \n",
    "    # Mover BRAKE al index -1 (valor auxiliar par no sobreescribir datos)\n",
    "    Y_pred_aux = Y_pred.copy()\n",
    "    Y_test_aux = Y_test.copy()\n",
    "    np.place(Y_pred_aux, Y_pred_aux == 1, -1)\n",
    "    np.place(Y_test_aux, Y_test_aux == 1, -1)\n",
    "    # Mover ACCELERATE al index 1\n",
    "    np.place(Y_pred_aux, Y_pred_aux == 0, 1)\n",
    "    np.place(Y_test_aux, Y_test_aux == 0, 1)\n",
    "    # Mover RIGHT_ACCELERATE al index 0\n",
    "    np.place(Y_pred_aux, Y_pred_aux == 5, 0)\n",
    "    np.place(Y_test_aux, Y_test_aux == 5, 0)\n",
    "    # Mover BRAKE al index 5\n",
    "    np.place(Y_pred_aux, Y_pred_aux == -1, 5)\n",
    "    np.place(Y_test_aux, Y_test_aux == -1, 5)\n",
    "    \n",
    "    # Calcula el Error Cuadrático Medio (MSE)\n",
    "    mse = mean_squared_error(Y_test_aux, Y_pred_aux)\n",
    "    print(\"MSE:\", mse)\n",
    "    \n",
    "\n",
    "# 0: ACCELERATE\n",
    "# 1: BRAKE\n",
    "# 2: LEFT_ACCELERATE\n",
    "# 3: LEFT_BRAKE\n",
    "# 4: NONE\n",
    "# 5: RIGHT_ACCELERATE\n",
    "# 6: RIGHT_BRAKE\n",
    "\n",
    "# EJERCICIO 4 - Prueba diferentes modelos de Machine Learning\n",
    "\n",
    "Y_test = Y_test.to_numpy()\n",
    "Y_test_floats = np.array(Y_test, dtype=float)\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "#========================================================================================================\n",
    "#                                             SIMILARITY\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Crear el modelo de vecinos más cercanos\n",
    "model = NearestNeighbors(n_neighbors=7, algorithm='auto')\n",
    "model.fit(X)\n",
    "# Punto de consulta para encontrar vecinos cercanos\n",
    "query_point = np.array([[-0.22401113, 2.40533218, -0.06294622, 0.18585261, 0.60490084]])\n",
    "#query_point = X_test\n",
    "# Encontrar los vecinos más cercanos\n",
    "distances, indexes = model.kneighbors(query_point)\n",
    "\n",
    "print(\"\\n ===================  SIMILARITY  ===================== \\n\")\n",
    "\n",
    "# Imprimir los vecinos más cercanos y sus distancias\n",
    "print(\"Vecinos más cercanos:\")\n",
    "print(X[indexes])\n",
    "print(\"Distancias:\")\n",
    "print(distances)\n",
    "print(\"Prediccion de los vecinos mas cercanos:\")\n",
    "\n",
    "Y_values = np.argmax(Y, axis=1)\n",
    "\n",
    "for index_pair in indexes:\n",
    "    for index in index_pair:\n",
    "        print(Y_values[index], end=\" \")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#for index in range(len(indexes)):\n",
    "#    print(Y[indexes[index]], \",\")\n",
    "\n",
    "#========================================================================================================\n",
    "#                                                SVM\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Polinómico: (kernel='poly', degree=degree)\n",
    "# Radial basis function (RBF o Gaussiano): (kernel='rbf')\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "\n",
    "SVC_Y_train = np.argmax(Y, axis=1)\n",
    "classifier = classifier.fit(X, SVC_Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\n ===================  SVM  ===================== \\n\")\n",
    "\n",
    "PrintData(Y_pred, Y_test)\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "#                                            DECISION TREE\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=4)\n",
    "\n",
    "# Entrenar modelo\n",
    "decision_tree = decision_tree.fit(X, Y)\n",
    "\n",
    "# DESCOMENTAR PARA PRINTEAR EL ARBOL GENERADO\n",
    "### # Los nombres de los 5 rayos/parametros\n",
    "### ray_features = [\"ray1\", \"ray2\", \"ray3\", \"ray4\", \"ray5\"]\n",
    "### r = export_text(decision_tree, feature_names=ray_features)\n",
    "### print(r)\n",
    "### decision_tree.score(X_test, Y_test)\n",
    "\n",
    "# Predice las clases en el conjunto de prueba\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "#print(\"Y_pred = \", Y_pred[:100])\n",
    "\n",
    "\n",
    "print(\"\\n ===================  DECISION TREE  ===================== \\n\")\n",
    "\n",
    "#Y_pred_floats = np.array(Y_pred, dtype=float)\n",
    "\n",
    "# Convertir los datos de formato [[true, false, ...], [false, true, ...]] a [0, 2, 2, 1, 0, 0 ...]\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "PrintData(Y_pred, Y_test)\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "#                                          RANDOM FOREST\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supongamos que X e Y son tus datos de entrenamiento\n",
    "# Supongamos que X_test e Y_test son tus datos de validación\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "random_forest_model.fit(X, Y)\n",
    "\n",
    "# Realizar predicciones en los datos de validación\n",
    "Y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "print(\"\\n ===================  RANDOM FOREST  ===================== \\n\")\n",
    "\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "PrintData(Y_pred, Y_test)\n",
    "\n",
    "#========================================================================================================\n",
    "#                                                MLP\n",
    "#========================================================================================================\n",
    "\n",
    "# Entrenar modelo con backpropagation\n",
    "\n",
    "neuronsInputLayer = X.shape[1]\n",
    "neuronsFirstHiddenLayer = 7\n",
    "neuronsSecondHiddenLayer = 7\n",
    "neuronsThirdHiddenLayer = 7\n",
    "neuronsOutputLayer = Y.shape[1]\n",
    "\n",
    "e = 0.12\n",
    "iterations = 5000\n",
    "myAlpha = 0.5\n",
    "myLambda = 0.5\n",
    "\n",
    "np.random.seed(2)\n",
    "thetas = []\n",
    "# Valores aleatorios en theta1 y theta2\n",
    "thetas.append(np.random.uniform(low=-e, high=e, size=( neuronsFirstHiddenLayer, neuronsInputLayer+1)))\n",
    "thetas.append(np.random.uniform(low=-e, high=e, size=( neuronsSecondHiddenLayer, neuronsFirstHiddenLayer+1)))\n",
    "#thetas.append(np.random.uniform(low=-e, high=e, size=( neuronsThirdHiddenLayer, neuronsSecondHiddenLayer+1)))\n",
    "thetas.append(np.random.uniform(low=-e, high=e, size=(neuronsOutputLayer, neuronsSecondHiddenLayer+1)))\n",
    "#theta3 = np.random.uniform(low=-e, high=e, size=(99, +1))\n",
    "#theta4 = np.random.uniform(low=-e, high=e, size=(99, +1))\n",
    "\n",
    "# Ajustar thetha1 y theta2 teniendo en cuenta los parametros dados\n",
    "thetas = iterateThetas(thetas, X, Y, iterations, myLambda, myAlpha)\n",
    "\n",
    "#ExportThetasToFile(thetas, \"../Assets/modelo1.txt\")\n",
    "# En modelo2 se guarda el sistema entrenado\n",
    "CreateTxt(thetas, \"../Assets/MLP_data.txt\")\n",
    "\n",
    "#=======================================================================================================\n",
    "#                             COMPARAR LOS RESULTADOS DE MLP CON SKLEARN\n",
    "#=======================================================================================================\n",
    "\n",
    "print(\"\\n ===================  MLP  ===================== \\n\" )\n",
    "# Entrenar a SKLearn con los mismos valores\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "sklearn_neural_network = MLPClassifier(\n",
    "    alpha=myLambda, \n",
    "    learning_rate_init=myAlpha, \n",
    "    activation='relu',\n",
    "    hidden_layer_sizes=(neuronsFirstHiddenLayer, neuronsSecondHiddenLayer),\n",
    "    max_iter = iterations\n",
    ")\n",
    "# Diferentes funciones de activacion\n",
    "# Relu es mejor para mas capas ocultas\n",
    "# activation='relu'\n",
    "# La sigmoidal es mejor para menos capas\n",
    "# activation='logistic'\n",
    "\n",
    "# Entrenamiento del MLP\n",
    "sklearn_neural_network.fit(X, Y)\n",
    "scores = cross_val_score(sklearn_neural_network, X, Y, cv=5, scoring=make_scorer(mean_squared_error))\n",
    "\n",
    "print(\"scores = \", scores )\n",
    "\n",
    "## Realizar la propagación hacia adelante\n",
    "\n",
    "# Predicciones con MLPClassifier\n",
    "layerValues_sklearn = sklearn_neural_network.predict(X_test)\n",
    "sklearnPredictions = np.argmax(layerValues_sklearn, axis=1)\n",
    "#print(\"SklearnPredictions:\", sklearnPredictions[:1000])\n",
    "\n",
    "layerValues, weighted_inputs = forwardprop(thetas, X_test)\n",
    "#print(\"layerValues[-1].shape: \", layerValues[-1].shape)\n",
    "#print(\"Y_test.shape: \", Y_test.shape)\n",
    "#print(\"Y_test: \", Y_test[:100])\n",
    "\n",
    "myPredictions = np.argmax(layerValues[-1], axis=1)\n",
    "\n",
    "print(\"Our MLP:\")\n",
    "PrintData(myPredictions, Y_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"SKL MLP:\")\n",
    "PrintData(sklearnPredictions, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf31d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from knn import predict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"data/Kart06.csv\", header=None)\n",
    "\n",
    "# Realizar one-hot encoding para la columna \"accion\"\n",
    "one_hot_encoded = pd.get_dummies(data[9], prefix=\"accion\")\n",
    "\n",
    "# Añadir los valores generados del one_hot_encoded al dataset\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)\n",
    "data = data.drop([9], axis=1)\n",
    "\n",
    "X = data.iloc[:, :9].to_numpy()\n",
    "Y = data.iloc[:, 9:].to_numpy()\n",
    "\n",
    "# mismo problema que en unity, detectaria distancia 0 y -1 como mas parecidos a 0 y 2\n",
    "np.place(X, X == -1, 100)\n",
    "\n",
    "\n",
    "#========DIVIDIR DATOS ENTRE ENTRENAMIENTO Y DE TEST==========================\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, train_size = 0.8, random_state = 1234)\n",
    "\n",
    "Y = np.argmax(Y, axis=1)\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "predictions = np.zeros(X_test.shape[0], dtype=int)\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    p = predict(X, Y, X_test[i])\n",
    "    predictions[i] = p\n",
    "\n",
    "print(\"Our KNN:\")\n",
    "PrintData(predictions, Y_test)\n",
    "print(\"\\n\")\n",
    "    \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnClssifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knnClssifier.fit(X, Y)\n",
    "\n",
    "skPredictions = knnClssifier.predict(X_test)\n",
    "\n",
    "print(\"SKL KNN:\")\n",
    "PrintData(skPredictions, Y_test)\n",
    "print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4c4f9",
   "metadata": {},
   "source": [
    "\n",
    "#========================================================================================================\n",
    "#                                RAZONAMIENTO FINAL\n",
    "#========================================================================================================\n",
    "\n",
    "Tras analizar las metricas y matrices de confusion obtenidas para cada uno de los siguientes modelos:\n",
    "\n",
    "SVM, DECISION TREE, RANDOM FOREST, MLP. KNN\n",
    "\n",
    "Creemos que el mejor modelo en este caso es el MLP.\n",
    "Aunque es notable que a medida que se le incluyen mas capas ocultas, es menos preciso y los resultados observables en Unity son peores.\n",
    "(En Unity, en la carpeta Assets, esta el MLP generado con 2 capas ocultas(MLP_data.txt) y con una sola (MLP_goodData.txt), la diferencia en conduccion entre los dos es notable).\n",
    "\n",
    "// KNN\n",
    "El modelo de KNN esta implementado en este documento tanto con sklearn como una implementacion propia.\n",
    "Tambien en Unity, se ha implementado el modelo de KNN, teniendo como base de datos el archivo KNNData.cvs\n",
    "\n",
    "Comparando estos dos modelos en la carrera, podemos observar que aunque KNN completa antes el circuito, si se usa el modelo de MLP con una sola capa oculta (MLP_goodData.txt), dicho modelo es mucho mas eficiente que KNN.\n",
    "\n",
    "Ademas, KNN y Similarity son sensibles al ruido, lo que es un problema, ya que los casos almacenados pueden contener informacion erronea en algunos instantes de la grabacion, y puede aplicar informacion que se grabo en una situacion especifica en un contexto general, sobretodo usando exclusivamente los datos de los raycast en nuestro caso.\n",
    "Como puede haber muchos mas casos en un videojuego de este estilo, este modelo puede llegar a ralentizar el juego.\n",
    "\n",
    "// SVM\n",
    "Este modelo, aunque genera un 72% de accuracity, no es la mejor opcion en este caso concreto, ya que por un lado no necesitamos un modelo que sea capaz de manejar muchas dimensiones, ya que solo tenemos en cuenta los 5 raycast. Mientras que por otro lado no es eficiente en grandes conjuntos de datos, y un videojuego de este estilo, la grabacion de numerosos datos es esencial.\n",
    "\n",
    "// DECISION TREE\n",
    "Aunque este modelo sea facil de interpretar y visualizar, y no requiera normalizacion de datos, es propenso al sobreajuste, lo que no es nada ideal en un videojuego de carreras si se quisiese extender la pista o cambiarla.\n",
    "\n",
    "// RANDOM FOREST\n",
    "Este modelo es mas flexible que el decision tree en cuanto a evitar el sobreajuste, aun asi, es sensible al ruido en los datos de entrenamiento, lo que puede suponer un problema.\n",
    "Tambien es mas lento en comparacion a modelos mas simples, lo que es aceptable hasta cierto punto ya que en un videojuego de carreras el tiempo de reaccion es esencial.\n",
    "\n",
    "// MLP\n",
    "El Perceptron Multicapa es el modelo que creemos que es mejor para este caso, ya que es capaz de aprender patrones complejos.\n",
    "Otro punto a favor es que se adapta bien cuando hay muchos datos de entrenamiento, aunque tome una considerable suma de tiempo.\n",
    "En este caso, existen muchos datos de entrenamiento, y podria haber muchos mas, por lo que es razonable elegir un modelo que se ajuste a estas condiciones.\n",
    "Aunque el modelo es bastante sensible a la inicialización de pesos y parámetros (hemos comprobado que el resultado obtenido en Unity viendo el coche varia bastante entre modelos identicos estructuralmente pero que se entrenaron con valores iniciales diferentes), podemos controlar dicha desventaja manipulando la semilla actual con \"np.random.seed(2)\".\n",
    "\n",
    "Aunque opinamos que el MLP es la mejor opcion basandonos en las metricas calculadas de los modelos, esto es solo en caso de que el MLP tenga solo una capa oculta, ya que mientras mas capas ocultas tenga peores son los resultados, tando en accuracy como en conduccion en Unity, si usamos la funcion de activacion de ReLu para las capas ocultas, los resultados mejoran, pero siguen sin superar a un modelo de menos capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a40afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e075f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

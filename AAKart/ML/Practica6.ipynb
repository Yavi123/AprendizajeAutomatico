{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c47028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===================  SIMILARITY  ===================== \n",
      "\n",
      "Vecinos más cercanos:\n",
      "[[[-0.22401113  2.40533218 -0.06294622  0.18585261  0.60490084]\n",
      "  [-0.22401113  1.71132124 -0.16463975  0.24222984  0.50889491]\n",
      "  [-0.22401113  1.51356572 -0.26135002  0.27972592  0.57493224]\n",
      "  [-0.22401113  1.52922684  0.04678098  0.20690589  0.15856397]\n",
      "  [-0.22401113  1.9973181   0.42763547  0.07368163 -0.20325688]\n",
      "  [-0.22401113  1.65982717  0.53778804  0.0176433   0.23137845]\n",
      "  [-0.22401113  1.49114722  0.13542706  0.1908588   0.03006357]]]\n",
      "Distancias:\n",
      "[[3.30725391e-09 7.10202991e-01 9.18869854e-01 9.89575906e-01\n",
      "  1.03578333e+00 1.04138020e+00 1.09797497e+00]]\n",
      "Prediccion de los vecinos mas cercanos:\n",
      "2 2 0 0 2 2 0 \n",
      "\n",
      "\n",
      " ===================  SVM  ===================== \n",
      "\n",
      "Accuracy :  72.75862068965517 %\n",
      "Matriz de Confusión:\n",
      "[[418   0  25   0   0   7   0]\n",
      " [  1   0   0   0   0   0   0]\n",
      " [168   0 194   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0]\n",
      " [ 24   0   0   0   0  21   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[418  32]\n",
      " [202 218]]\n",
      "\n",
      " ===================  DECISION TREE  ===================== \n",
      "\n",
      "Accuracy :  80.11494252873564 %\n",
      "Matriz de Confusión:\n",
      "[[387   0  58   0   0   5   0]\n",
      " [  1   0   0   0   0   0   0]\n",
      " [ 73   0 289   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0]\n",
      " [ 13   0  11   0   0  21   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[387  63]\n",
      " [ 96 324]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ann import costL2, backprop, forwardprop, iterateThetas\n",
    "\n",
    "from Utils import ExportThetasToFile\n",
    "from Utils import CreateTxt\n",
    "\n",
    "#print(pd.__version__)\n",
    "\n",
    "# EJERCICIO 2 - visualiza los datos\n",
    "\n",
    "# Cargar los datos desde el archivo CSV\n",
    "data = pd.read_csv(\"data/kartAllData.csv\", header=None, names=[\"dist1\", \"dist2\", \"dist3\", \"dist4\", \"dist5\", \n",
    "                                                           \"x\", \"y\", \"z\", \"tiempo\", \"accion\"])\n",
    "\n",
    "# Eliminar las columnas de posición y tiempo ya que consideramos que no son tan relevantes como la distancia de los raycast\n",
    "droppingPosition = True\n",
    "\n",
    "if (droppingPosition):\n",
    "    cleanDataset = data.drop([\"x\", \"y\", \"z\", \"tiempo\"], axis=1)\n",
    "    numInputs = 5\n",
    "else:\n",
    "    cleanDataset = data.drop([\"y\", \"tiempo\"], axis=1)\n",
    "    numInputs = 7\n",
    "    \n",
    "# Realizar one-hot encoding para la columna \"accion\"\n",
    "one_hot_encoded = pd.get_dummies(data[\"accion\"], prefix=\"accion\")\n",
    "\n",
    "# Añadir los valores generados del one_hot_encoded al dataset\n",
    "cleanDataset = pd.concat([cleanDataset, one_hot_encoded], axis=1)\n",
    "cleanDataset = cleanDataset.drop([\"accion\"], axis=1)\n",
    "\n",
    "# Utilizaremos los valores de las distancias de los 5 raycast como input\n",
    "X = cleanDataset.iloc[:, :numInputs]\n",
    "# Mientras que utilizaremos los 7 estados de accion, pasados por one_hot_encoded\n",
    "Y = cleanDataset.iloc[:, numInputs:]\n",
    "\n",
    "#==Normalizacion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaling=StandardScaler()\n",
    "scaling.fit(X)\n",
    "X=scaling.transform(X)\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "#                    DIVIDIR DATOS ENTRE ENTRENAMIENTO Y DE TEST\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, train_size = 0.8, random_state = 1234)\n",
    "\n",
    "\n",
    "# Visualizar la distribución de clases\n",
    "class_distribution = data[\"accion\"].value_counts()\n",
    "\n",
    "# Graficar la distribución de clases\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribución de Clases')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Frecuencia')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# Los valores de Y_pred y Y_test tienen que tener el siguiente formato:\n",
    "# Y_pred =  [0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0  ... 0 0 0 0 0 2 0] \n",
    "def PrintData(Y_pred, Y_test) :\n",
    "    # Calcula el accuracy\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    print(\"Accuracy : \", acc * 100, \"%\")\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    Y_test_np = np.array(Y_test, dtype=int)\n",
    "    Y_pred_np = np.array(Y_pred, dtype=int)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(Y_test_np, Y_pred_np, labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "    \n",
    "    print(\"Matriz de Confusión:\")\n",
    "    print(conf_matrix)\n",
    "    Y_test_np = np.array(Y_test, dtype=bool)\n",
    "    Y_pred_np = np.array(Y_pred, dtype=bool)\n",
    "    \n",
    "    conf_matrix_bool = confusion_matrix(Y_test_np.flatten(), Y_pred_np.flatten())\n",
    "    \n",
    "    print(\"Matriz de Confusión en bools:\")\n",
    "    print(conf_matrix_bool)\n",
    "    \n",
    "\n",
    "\n",
    "# EJERCICIO 4 - Prueba diferentes modelos de Machine Learning\n",
    "\n",
    "Y_test = Y_test.to_numpy()\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "#========================================================================================================\n",
    "#                                             SIMILARITY\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Crear el modelo de vecinos más cercanos\n",
    "model = NearestNeighbors(n_neighbors=7, algorithm='auto')\n",
    "model.fit(X)\n",
    "# Punto de consulta para encontrar vecinos cercanos\n",
    "query_point = np.array([[-0.22401113, 2.40533218, -0.06294622, 0.18585261, 0.60490084]])\n",
    "#query_point = X_test\n",
    "# Encontrar los vecinos más cercanos\n",
    "distances, indexes = model.kneighbors(query_point)\n",
    "\n",
    "print(\"\\n ===================  SIMILARITY  ===================== \\n\")\n",
    "\n",
    "# Imprimir los vecinos más cercanos y sus distancias\n",
    "print(\"Vecinos más cercanos:\")\n",
    "print(X[indexes])\n",
    "print(\"Distancias:\")\n",
    "print(distances)\n",
    "print(\"Prediccion de los vecinos mas cercanos:\")\n",
    "\n",
    "Y_values = np.argmax(Y, axis=1)\n",
    "\n",
    "for index_pair in indexes:\n",
    "    for index in index_pair:\n",
    "        print(Y_values[index], end=\" \")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#for index in range(len(indexes)):\n",
    "#    print(Y[indexes[index]], \",\")\n",
    "\n",
    "#========================================================================================================\n",
    "#                                                SVM\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "\n",
    "SVC_Y_train = np.argmax(Y, axis=1)\n",
    "classifier = classifier.fit(X, SVC_Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\n ===================  SVM  ===================== \\n\")\n",
    "\n",
    "PrintData(Y_pred, Y_test)\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "#                                          ARBOL DE DECISION\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=4)\n",
    "\n",
    "# Entrenar modelo\n",
    "decision_tree = decision_tree.fit(X, Y)\n",
    "\n",
    "# DESCOMENTAR PARA PRINTEAR EL ARBOL GENERADO\n",
    "### # Los nombres de los 5 rayos/parametros\n",
    "### ray_features = [\"ray1\", \"ray2\", \"ray3\", \"ray4\", \"ray5\"]\n",
    "### r = export_text(decision_tree, feature_names=ray_features)\n",
    "### print(r)\n",
    "### decision_tree.score(X_test, Y_test)\n",
    "\n",
    "# Predice las clases en el conjunto de prueba\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "#print(\"Y_pred = \", Y_pred[:100])\n",
    "\n",
    "\n",
    "print(\"\\n ===================  DECISION TREE  ===================== \\n\")\n",
    "\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "PrintData(Y_pred, Y_test)\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "#                                          RANDOM FOREST\n",
    "#========================================================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supongamos que X e Y son tus datos de entrenamiento\n",
    "# Supongamos que X_test e Y_test son tus datos de validación\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "random_forest_model.fit(X, Y)\n",
    "\n",
    "# Realizar predicciones en los datos de validación\n",
    "Y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "print(\"\\n ===================  RANDOM FOREST  ===================== \\n\")\n",
    "\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "PrintData(Y_pred, Y_test)\n",
    "\n",
    "#========================================================================================================\n",
    "#                                                MLP\n",
    "#========================================================================================================\n",
    "\n",
    "# Entrenar modelo con backpropagation\n",
    "\n",
    "neuronsInputLayer = X.shape[1]\n",
    "neuronsFirstHiddenLayer = 6\n",
    "neuronsSecondHiddenLayer = 9\n",
    "neuronsThirdHiddenLayer = 7\n",
    "neuronsOutputLayer = Y.shape[1]\n",
    "\n",
    "e = 0.12\n",
    "iterations = 5000\n",
    "myAlpha = 0.5\n",
    "myLambda = 0.5\n",
    "\n",
    "thetas = []\n",
    "# Valores aleatorios en theta1 y theta2\n",
    "thetas.append(np.random.uniform(low=-e, high=e, size=( neuronsFirstHiddenLayer, neuronsInputLayer+1)))\n",
    "thetas.append(np.random.uniform(low=-e, high=e, size=( neuronsSecondHiddenLayer, neuronsFirstHiddenLayer+1)))\n",
    "thetas.append(np.random.uniform(low=-e, high=e, size=( neuronsThirdHiddenLayer, neuronsSecondHiddenLayer+1)))\n",
    "thetas.append(np.random.uniform(low=-e, high=e, size=(neuronsOutputLayer, neuronsThirdHiddenLayer+1)))\n",
    "#theta3 = np.random.uniform(low=-e, high=e, size=(99, +1))\n",
    "#theta4 = np.random.uniform(low=-e, high=e, size=(99, +1))\n",
    "\n",
    "## Ajustar thetha1 y theta2 teniendo en cuenta los parametros dados\n",
    "#thetas = iterateThetas([theta1, theta2], X, Y, iterations, myLambda, myAlpha)\n",
    "\n",
    "\n",
    "#=======================================================================================================\n",
    "#                             COMPARAR LOS RESULTADOS DE MLP CON SKLEARN\n",
    "#=======================================================================================================\n",
    "\n",
    "\n",
    "print(\"\\n ===================  MLP  ===================== \\n\" )\n",
    "# Entrenar a SKLearn con los mismos valores\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "hidden_layer_sizes = (neuronsFirstHiddenLayer)\n",
    "sklearn_neural_network = MLPClassifier(\n",
    "    alpha=myLambda, \n",
    "    learning_rate_init=myAlpha, \n",
    "    activation='logistic',\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    max_iter = iterations\n",
    ")\n",
    "sklearn_neural_network.fit(X, Y)\n",
    "\n",
    "\n",
    "# Ajustar thetha1 y theta2 teniendo en cuenta los parametros dados\n",
    "thetas = iterateThetas(thetas, X, Y, iterations, myLambda, myAlpha)\n",
    "\n",
    "#ExportThetasToFile(thetas, \"../Assets/modelo1.txt\")\n",
    "CreateTxt(thetas, \"../Assets/modelo2.txt\")\n",
    "\n",
    "\n",
    "## Realizar la propagación hacia adelante\n",
    "\n",
    "# Predicciones con MLPClassifier\n",
    "layerValues_sklearn = sklearn_neural_network.predict(X_test)\n",
    "sklearnPredictions = np.argmax(layerValues_sklearn, axis=1)\n",
    "#print(\"SklearnPredictions:\", sklearnPredictions[:1000])\n",
    "\n",
    "layerValues, weighted_inputs = forwardprop(thetas, X_test)\n",
    "#print(\"layerValues[-1].shape: \", layerValues[-1].shape)\n",
    "#print(\"Y_test.shape: \", Y_test.shape)\n",
    "#print(\"Y_test: \", Y_test[:100])\n",
    "\n",
    "myPredictions = np.argmax(layerValues[-1], axis=1)\n",
    "\n",
    "print(\"Our MLP:\")\n",
    "PrintData(myPredictions, Y_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"SKL MLP:\")\n",
    "PrintData(sklearnPredictions, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecaf31d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our KNN:\n",
      "Accuracy :  77.04918032786885 %\n",
      "Matriz de Confusión:\n",
      "[[29  0  1  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 6  0 14  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  0  0]\n",
      " [ 2  0  3  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[29  3]\n",
      " [ 8 21]]\n",
      "\n",
      "\n",
      "SKL KNN:\n",
      "Accuracy :  72.1311475409836 %\n",
      "Matriz de Confusión:\n",
      "[[28  0  3  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 7  0 13  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  0  0]\n",
      " [ 3  0  3  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "Matriz de Confusión en bools:\n",
      "[[28  4]\n",
      " [10 19]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from knn import predict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"data/Kart06.csv\", header=None)\n",
    "\n",
    "# Realizar one-hot encoding para la columna \"accion\"\n",
    "one_hot_encoded = pd.get_dummies(data[9], prefix=\"accion\")\n",
    "\n",
    "# Añadir los valores generados del one_hot_encoded al dataset\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)\n",
    "data = data.drop([9], axis=1)\n",
    "\n",
    "X = data.iloc[:, :9].to_numpy()\n",
    "Y = data.iloc[:, 9:].to_numpy()\n",
    "\n",
    "# mismo problema que en unity, detectaria distancia 0 y -1 como mas parecidos a 0 y 2\n",
    "np.place(X, X == -1, 100)\n",
    "\n",
    "\n",
    "#========DIVIDIR DATOS ENTRE ENTRENAMIENTO Y DE TEST==========================\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, train_size = 0.8, random_state = 1234)\n",
    "\n",
    "Y = np.argmax(Y, axis=1)\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "predictions = np.zeros(X_test.shape[0], dtype=int)\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    p = predict(X, Y, X_test[i])\n",
    "    predictions[i] = p\n",
    "\n",
    "print(\"Our KNN:\")\n",
    "PrintData(predictions, Y_test)\n",
    "print(\"\\n\")\n",
    "    \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnClssifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knnClssifier.fit(X, Y)\n",
    "\n",
    "skPredictions = knnClssifier.predict(X_test)\n",
    "\n",
    "print(\"SKL KNN:\")\n",
    "PrintData(skPredictions, Y_test)\n",
    "print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a40afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e075f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
